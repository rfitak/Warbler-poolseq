# Sequence Read Processing
The bioinformatic analysis of the raw sequencing reads is detailed below. Links to papers and/or specific programs are provided when necessary. The version of the various software used is also provided. Some analyses utilized custom, short scripts which are provides as needed. All analyses were run on the *coombs* computer cluster at the University of Central Florida (UCF) unless otherwise stated. *Coombs* is a Linux server currently running **Ubuntu 20.04.6 LTS**.

According to the sequencing provider _Novogene Coroporation Inc._, the following initial quality filters were applied prior to sending the raw sequence data:
1. Reads containing adapters were removed.
2. Reads containing N > 10% (N represents the base cannot be determined) were removed.
3. Reads containing >50% low quality (Qscore<= 5) bases were removed.
 
According to the sequencing provider *Novogene*, the sequencing adapters below were used:
- **5' Adapter:** 5'-AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT-3'
- **3' Adapter:** 5'-GATCGGAAGAGCACACGTCTGAACTCCAGTCACGGATGACTATCTCGTATGCCGTCTTCTGCTTG-3'

###  *Step 1: Inspect the raw data*
The sequencing data were generated by _Novogene_, as part of projects **Davis-US-UCF-2-Acrocephalus arundinaceus-WGS-30G-WOBI-NVUS2023051780** (Spring pools) and **Davis-US-UCF-2-Acrocephalus arundinaceus-WGS-PCR free-Novaseq6000-30G-WOBI-NVUS2024021529** (Autumn pools). The final reports summarizing all the sequencing can be found separately in the files for [**Spring pools**](./data/Report_X202SC23053936-Z01-F001_20230714155418_20230715070021.zip) and [**Autumn pools**](./data/Report_X202SC24023681-Z01-F001_20240326181108.zip). The sequencing was performed on an Illumina Novoseq 6000 and the raw data are summarized in the table below:

| Pool | Library_FlowCell_Lane | # Reads | Raw Data (GB) |
| --- | --- | --- | --- |
|  Early Spring (B1)   | CKDN230018353-1A_HCVG2DSX7_L4 | 221,175,638 | 33.2 |
|  Late Spring (B2)    | CKDN230018354-1A_HCNCHDSX7_L2 | 60,738,810 | 9.1 |
|  Late Spring (B2)    | CKDN230018354-1A_HCLYYDSX7_L4 | 191,954,586 | 28.8 |
|  Early Autumn (C1)   | CKDN240003772-1A_H5VJTDSXC_L1 | 211,991,046 | 31.8 |
|  Early Autumn (C1)   | CKDN240003772-1A_H5TJJDSXC_L3 | 71,357,896 | 10.7 |
|  Late Autumn (C2)    | CKDN240003773-1A_H5VJTDSXC_L1 | 177,416,198 | 26.6 |
|  Late Autumn (C2)    | CKDN240003773-1A_H5TJJDSXC_L3 | 69,045,404 | 10.4 |

The raw data files were confirmed to be uncorrupted by checking the md5 tags as follows (all files were confirmed "**OK**"):

``` bash
# Check MD5 tags
md5sum -c usftp21.novogene.com
   # usftp21.novogene.com is the folder from novogene containing all the raw data.
```



A file called `samples.txt` was generated to store a list of all 66 purple martin sample names:

```bash
# Generate list of sample names for use later
ls usftp21.novogene.com/01.RawData > samples.txt
```



######  *Step 2: Prepare the Progne subis reference genome*

The reference genome was generated by [de Greef et al. 2023](https://doi.org/10.1038/s41598-023-29470-7) as part of a resequencing study focusing on their migration.  The reference genome can be found in NCBI under accession [GCA_022316685.1](https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_022316685.1/).

```bash
# Download the reference genome from NCBI's FTP site
wget \
https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/022/316/685/GCA_022316685.1_UM_PSub_1/GCA_022316685.1_UM_PSub_1_genomic.fna.gz

# Uncompress the file and change its name
gunzip GCA_022316685.1_UM_PSub_1_genomic.fna.gz
mv GCA_022316685.1_UM_PSub_1_genomic.fna Psubis.fa

# Report basic sequencing statistics
seqstats Psubis.fa
   #   Total n:	2896
   #   Total seq:	1165951862 bp
   #   Avg. seq:	402607.69 bp
   #   Median seq:	77068.50 bp
   #   N 50:		6129949 bp
   #   Min seq:	16249 bp
   #   Max seq:	45082031 bp
   
# Index the reference genome
bwa index Psubis.fa
```



###### *Step 3: Clean and Map the Reads*

The reads were cleaned and mapped to the reference genome. The in-house pipeline, `supermapper` v5.3 was used for this process. `supermapper` cleans all the reads (including adapter removal) using [*fastp v0.20.0*](https://github.com/OpenGene/fastp) ([Chen et al. 2018](https://doi.org/10.1093/bioinformatics/bty560)) then maps then to the reference using [*bwa*](https://bio-bwa.sourceforge.net). The resulting BAM files are sorted, duplicates removed, then conservatively cleaned to remove improper reads. Once mapping was complete, the summary stats were generated from the resulting *BAM* files (`${name}.bamstats`). Additional details of various trimming and cleaning parameters are detailed below the code. The `supermapper` command is below, then the additional details within each step of the `supermapper` pipeline are detailed afterwards.

```bash
# Run supermapper (${name} references each of the samples in samples.txt)
name="TX_234"
supermapper \
	-i ${name}_1.fq.gz \
	-j ${name}_2.fq.gz \
	-r Psubis.fa \
	-c \
	-t 16 \
	-g "@RG\tID:${name}\tSM:${name}\tPL:illumina\tLB:run1" \
	-o $name
```



_clean the raw reads using fastp_

```bash
  # run Fastp
     # ${fwd} = forward reads file
     # ${rev} = reverse reads file
     # ${adapters} = fasta file of adapters to remove
     # ${out} = output file basename
     # ${threads} = # CPU threads to use
     # note that the results are piped to standard out
  fastp \
      --in1=${fwd} \
      --in2=${rev} \
      --stdout \
      --adapter_fasta=${adapters} \
      --cut_front \
      --cut_tail \
      --cut_window_size=4 \
      --cut_mean_quality=20 \
      --qualified_quality_phred=20 \
      --average_qual=20 \
      --unqualified_percent_limit=30 \
      --n_base_limit=5 \
      --length_required=50 \
      --low_complexity_filter \
      --complexity_threshold=30 \
      --overrepresentation_analysis \
      --trim_poly_x \
      --poly_x_min_len=10 \
      --html=${out}.html \
      --json=${out}.json \
      --report_title="$out" \
      --thread=${threads} | # piped directly into bwa
```

- _Parameters Explained:_
  - ***--in1/--in2*** :: input forward and reverse read files, recognizes gzip

  - ***--stdout*** :: write to standard out for piping

  - ***--adapter_fasta file*** :: a file of known Illumina adapters to trim

  - ***--cut_front*** :: enable a 5' sliding window trimmer, like trimmomatic

  - ***--cut_tail*** :: enable a 3' sliding window trimmer, like trimmomatic

  - ***--cut_window_size=4*** :: window size for the trimming

  - ***--cut_mean_quality=20*** :: mean base score across the window required, or else trim the last base

  - ***--qualified_quality_phred=20*** :: minimum base quality score to keep

  - ***--average_qual=20*** :: remove read of the average quality across all bases is < 20

  - ***--unqualified_percent_limit=30*** :: Percent of bases allowed to be less than q in a read

  - ***--n_base_limit=5*** :: if one read's number of N bases is >5, then this read pair is discarded

  - ***--length_required=50*** :: minimum read length to keep after trimming

  - ***--low_complexity_filter*** :: filter sequences with a low complexity

  - ***--complexity_threshold=30*** :: threshold for sequence complexity filter

  - ***--overrepresentation_analysis*** :: look for overrepresented sequences, like adapters

  - ***--trim_poly_x*** :: trim strings of homopolymers at the 3' end of reads

  - ***--poly_x_min_len 10*** :: minimum length of homopolymer ot trim

  - ***--json=${out}.json*** :: output file name, JSON format

  - ***--html=${out}.html*** :: output file name, HTML format

  - ***--report_title="$out"*** :: output report tile

  - ***--thread=${threads}***  :: number of cpus to use



_mapping with bwa mem_

```bash
# run bwa mem
   # ${threads} = # CPU threads to use
   # Psubis.fa = indexed reference genome from above
bwa mem \
      -M \
      -p \
      -R "$rg" \
      -t ${threads} \
      Psubis.fa \
      - | # read input piped from fastp; then output piped into samtools
```

- _Parameters Explained:_

  - ***-M*** :: mark shorter split hits as secondary

  - ***-p*** :: smart pairing (ignoring in2.fq), in other words, data input are interleaved fastq

  - ***-r*** :: read groups ID line, in other words, sample name

  - ***-t*** :: number of cpus to use



_initial processing with samtools_

```bash
# Process with samtools
   # ${threads} = # CPU threads to use
   # ${out} = output file basename
samtools sort \
   -T ${out}.tmp \
   -n \
   -@ ${threads} \
   - | \
   samtools fixmate \
      -@ ${threads} \
      -m \
      - - | \
      samtools sort \
         -T ${out}.tmp2 \
         -O bam \
         -@ ${threads} \
         - | \
         samtools markdup \
             -T ${out}.tmp3 \
             -O bam \
             -@ ${threads} \
             - ${out}.sorted.bam
```

- _Parameters Explained:_
  - `sort -n` :: sort BAM file numerically
  - `fixmate -m` :: fixmates and add mate score tag
  - `markdup` :: mark PCR/optical duplicates for later removal.



_cleaning the BAM file_

```bash
# Clean up the bam file
samtools view \
      -b \
      -h \
      -q 20 \
      -f 0x2 \
      -F 0x4 \
      -F 0x8 \
      -F 0x400 \
      -@ ${threads} \
      -o ${out}.clean.sorted.bam \
      ${out}.sorted.bam
```

- _Parameters Explained:_

  - ***-b*** :: output BAM format

  - ***-h*** :: Include header in SAM output

  - ***-q*** :: remove reads with mapping quality < 20
  - ***-f 0x2*** :: keep reads mapped in proper pair
  - ***-F 0x4*** :: remove unmapped reads
  - ***-F 0x8*** :: remove read when its mate is unmapped
  - ***-F 0x400*** :: remove read when mate is mapped to the reverse strand, or not the primary alignment.



_post-process BAM file_

```bash
# Process BAM and get stats
   # ${threads} = # CPU threads to use
   # ${out} = output file basename
samtools index -@ ${threads} ${out}.clean.sorted.bam
samtools stats -@ ${threads} ${out}.clean.sorted.bam > ${out}.bamstats
```



_summarize all output in `data.summary.csv`_

```bash
# Printer table header
echo -e "ID,raw reads,raw bases,trimmed reads,trimmed bases,mapped reads,mapped bases,coverage" > data.summary.csv

# Loop to write to output for each bird sample
while read i
   do
   reads=$(grep -e "total reads:" -e "total bases:" ${i}/${i}.html | perl -ne '/>([0-9.]* [MG])/; print "$1\n"' | tr "\n" "," | perl -ne 'print "$_\n"')
   mapped=$(grep -e "reads mapped and paired:" -e "bases mapped:" ${i}/${i}.bamstats | cut -f3 | tr "\n" "," | perl -ne 'chomp(); @a=split /,/;$b = $a[1] / 1165951862; print "$_$b\n"')
   echo -e "${i},${reads}${mapped}" >> data.summary.csv
   done < samples.txt
```

